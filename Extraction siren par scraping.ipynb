{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 24 17:52:34 2022\n",
    "\n",
    "@author: asutter-adc\n",
    "\"\"\"\n",
    "\n",
    "#nouvelle version des fonctions d'identification et de recherche de siren, prends en compte les siren attachés. Même process de recherche que test4.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from re import compile\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from re import compile\n",
    "\n",
    "def is_luhn_valid(x: int) -> bool:\n",
    "    \"\"\"\n",
    "    Application de la formule de Luhn à un nombre\n",
    "    Permet la verification du numero SIREN et Siret d'un acheteur/etablissement\n",
    "    Retour:\n",
    "        - bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        luhn_corr = [0, 2, 4, 6, 8, 1, 3, 5, 7, 9]\n",
    "        list_number_in_x = [int(i) for i in list(str(x))]\n",
    "        l2 = [luhn_corr[i] if (index + 1) % 2 == 0 else i for index, i in enumerate(list_number_in_x[::-1])]\n",
    "        if sum(l2) % 10 == 0:\n",
    "            return True\n",
    "        elif str(x)[:9] == \"356000000\":  # SIREN de la Poste\n",
    "            if sum(list_number_in_x) % 5 == 0:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def isSIREN(c) :\n",
    "    if len(c) == 11 :\n",
    "        if c[0:3].isdigit() and c[3]==' ' and c[4:7].isdigit() and c[7]==' ' and c[8:].isdigit() :\n",
    "            return is_luhn_valid(''.join(c.split()))\n",
    "\n",
    "    elif len(c) == 9 :\n",
    "        return is_luhn_valid(c)\n",
    "\n",
    "    return False\n",
    "\n",
    "def findSIREN(chaine):\n",
    "\n",
    "    if len(chaine) < 14 :\n",
    "        return 'Pas de SIREN trouvé'\n",
    "\n",
    "    else :\n",
    "        #première étape : on recherche dans le document un SIREN sous le type ??? ??? ???\n",
    "        k=0\n",
    "\n",
    "        while k <= len(chaine)-11 :\n",
    "            if isSIREN(chaine[k:k+11]) :\n",
    "                return chaine[k:k+11]\n",
    "\n",
    "            k+=1\n",
    "\n",
    "        #deuxième étape : on recherche un numéro de TVA au format FR??SIREN\n",
    "        k= 0\n",
    "\n",
    "        while k<=len(chaine) -13 :\n",
    "            if chaine[k:k+2]=='FR' and chaine[k+2:k+13].isdigit() :\n",
    "                if is_luhn_valid(chaine[k+4:k+13]):\n",
    "                    return chaine[k+4:k+13]\n",
    "\n",
    "            k+=1\n",
    "\n",
    "        #troisième étape : on recherche un SIRET sans espace dont on extrait le SIREN, avec un espace le précédant (essentiel pour éviter les longues suites de chiffres présents dans les liens)\n",
    "        k=0\n",
    "\n",
    "        while k <= len(chaine)-16 :\n",
    "            if chaine[k+1:k+15].isdigit() and chaine[k]==' ':\n",
    "                if is_luhn_valid(chaine[k+1:k+15]) and is_luhn_valid(chaine[k+1:k+10]) :\n",
    "                    return chaine[k+1:k+10]\n",
    "\n",
    "            k+=1\n",
    "\n",
    "        #quatrième étape : on cherche un SIREN tout attaché avec espaces avant et après\n",
    "        k=0\n",
    "\n",
    "        while k<= len(chaine)-11 :\n",
    "\n",
    "            if chaine[k]==' ' and chaine[k+1:k+10].isdigit() and chaine[k+10]==' ':\n",
    "                if is_luhn_valid(chaine[k+1:k+10]) :\n",
    "                    return chaine[k+1:k+10]\n",
    "\n",
    "            elif chaine[k]==' ' and chaine[k+1:k+10].isdigit() and chaine[k+10]=='.':\n",
    "                if is_luhn_valid(chaine[k+1:k+10]) :\n",
    "                    return chaine[k+1:k+10]\n",
    "\n",
    "            k+=1\n",
    "\n",
    "\n",
    "    return 'Pas de SIREN trouvé'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstpages = ['Mentions légales', 'Mentions Légales', 'mentions légales', 'Legal Notice', 'Legal Notices', 'Legal notice', 'Legal notices', 'Legal', 'Legals', 'legal', 'legals', 'Terms', 'terms', 'Conditions Générales de Vente', 'Conditions générales de vente', 'Conditions Générales', 'Conditions générales', 'conditions générales' ,'CG', 'Conditions générales', 'Conditions de vente', 'Conditions de Vente', 'Mentions légales & CGU', 'CGUV', 'CGU', 'Terms of use', 'Terms of Use', 'Legal Information', 'Legal information', 'Legal Informations', 'Legal informations', 'Privacy', 'privacy', 'Privacy Policy', 'Privacy policy', 'Terms of service', 'Legal Mentions', 'Legal mentions', 'legal mentions', 'Legal Terms', 'Legal terms', 'About Us', 'About us', 'Disclaimer']\n",
    "\n",
    "\n",
    "\n",
    "nom = 'sitesDR2001.csv'\n",
    "siren = 'nomsirenDR2001.txt'\n",
    "\n",
    "file = open(nom, encoding = 'utf-8')\n",
    "newfile = open(siren, 'w', encoding = 'utf-8')\n",
    "i=1\n",
    "\n",
    "for ligne in file :\n",
    "    print(i)\n",
    "    lst = ligne.strip().split(';')\n",
    "    url = lst[0]\n",
    "    try :\n",
    "        if url !='' :\n",
    "\n",
    "            if url[-1]=='/' :\n",
    "                url=url[:-1]\n",
    "\n",
    "            res = requests.get(url)\n",
    "\n",
    "            soup=BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "            k = 0\n",
    "            length = len(lstpages)\n",
    "            sirentrouve=False\n",
    "            siren = 'Pages introuvables'\n",
    "            pageouverte=False\n",
    "\n",
    "\n",
    "            while k<length and not sirentrouve :\n",
    "\n",
    "                page = lstpages[k]\n",
    "                #premier test simple\n",
    "                link = soup.find('a', string=page, href=True)\n",
    "\n",
    "                if link is not None :\n",
    "                    pageouverte=True\n",
    "                    newurl = link.get('href')\n",
    "\n",
    "                    if url[8:10]=='ww' :\n",
    "                        test=url[12:-4]\n",
    "\n",
    "                    else :\n",
    "                        test = url[8:-4]\n",
    "\n",
    "                    if newurl.find(test) == -1 and newurl[:2]!='ht' and newurl[:2]!='//' : #cas où url relatif\n",
    "                        if url[-3:]=='/fr' and newurl[:3]=='/fr':\n",
    "                            newurl = url+newurl[3:]\n",
    "\n",
    "                        elif url[-4:]=='/fr/' and newurl[:3]=='/fr' :\n",
    "                            newurl = url + newurl[4:]\n",
    "\n",
    "                        else :\n",
    "                            newurl = url + newurl\n",
    "\n",
    "                    if newurl[:2]=='//': #cas où url non relatif mais ne commençant pas par http:\n",
    "                        newurl='http:'+newurl\n",
    "\n",
    "                    newres = requests.get(newurl)\n",
    "                    contenu = str(newres.content)\n",
    "\n",
    "                    #siren = findSIREN(contenu)\n",
    "\n",
    "                    #sirentrouve = isSIREN(siren)\n",
    "\n",
    "                    #if not sirentrouve : #gérons les espaces insécables\n",
    "\n",
    "                    soup2=BeautifulSoup(newres.content, 'html.parser')\n",
    "                    chaine2=str(soup2)\n",
    "\n",
    "                    chaine3=chaine2.replace('\\xa0', ' ')\n",
    "\n",
    "\n",
    "                    siren = findSIREN(chaine3)\n",
    "                    sirentrouve=isSIREN(siren)\n",
    "\n",
    "                k+=1\n",
    "\n",
    "            if not sirentrouve :\n",
    "\n",
    "                link=None\n",
    "                k=0\n",
    "                while k<length and not sirentrouve :\n",
    "\n",
    "                    page = lstpages[k]\n",
    "\n",
    "                #on teste ceux où l'encodage a été correct mais où la string est à striper\n",
    "                    if link is None :\n",
    "\n",
    "                        liens = soup.find_all('a', href =True, string = True)\n",
    "                        nbliens=len(liens)\n",
    "                        compt=1\n",
    "                        while compt <= nbliens and link is None :\n",
    "                            tag=liens[nbliens-compt]\n",
    "                            chaine=tag.string.strip()\n",
    "                            if chaine.find(page)!=-1 :\n",
    "                                link = tag\n",
    "\n",
    "                            compt += 1\n",
    "\n",
    "                #on teste ceux mal encodés, en misant sur le fait qu'ils sont encodés en latin-1\n",
    "                    if link is None :\n",
    "\n",
    "                        liens=soup.find_all('a', href=True, string=True)\n",
    "                        nbliens=len(liens)\n",
    "                        compt=1\n",
    "                        while compt <= nbliens and link is None :\n",
    "                            tag=liens[nbliens-compt]\n",
    "                            chaine=str(tag.string).strip()\n",
    "                            binaire= bytes(page, 'utf-8') #c'est la page qu'on décode\n",
    "                            testl=binaire.decode('latin-1')\n",
    "\n",
    "                            if chaine.find(testl)!=-1 : #on demande juste à ce que page se trouve dans string\n",
    "                                link = tag\n",
    "\n",
    "                            compt += 1\n",
    "\n",
    "\n",
    "\n",
    "                    if link is not None :\n",
    "\n",
    "                        newurl = link.get('href')\n",
    "\n",
    "                        if url[8:10]=='ww' :\n",
    "                            test=url[12:-4]\n",
    "\n",
    "                        else :\n",
    "                            test = url[8:-4]\n",
    "\n",
    "                        if newurl.find(test) == -1 and newurl[:2]!='ht' and newurl[:2]!='//': #cas où url relatif\n",
    "                            if url[-3:]=='/fr' and newurl[:3]=='/fr':\n",
    "                                newurl = url+newurl[3:]\n",
    "\n",
    "                            elif url[-4:]=='/fr/' and newurl[:3]=='/fr' :\n",
    "                                newurl = url + newurl[4:]\n",
    "\n",
    "                            else :\n",
    "                                newurl = url + newurl\n",
    "\n",
    "                        if newurl[:2]=='//':\n",
    "                            newurl='http:'+newurl\n",
    "\n",
    "                        newres = requests.get(newurl)\n",
    "                        contenu = str(newres.content)\n",
    "\n",
    "                        soup2=BeautifulSoup(newres.content, 'html.parser')\n",
    "                        chaine2=str(soup2)\n",
    "                        chaine3=chaine2.replace('\\xa0', ' ')\n",
    "\n",
    "                        siren = findSIREN(chaine3)\n",
    "                        sirentrouve=isSIREN(siren)\n",
    "\n",
    "                    k+=1\n",
    "\n",
    "        #cas où le string contient des strings\n",
    "            if not sirentrouve :\n",
    "\n",
    "                link=None\n",
    "                k=0\n",
    "                while k<length and not sirentrouve :\n",
    "\n",
    "                    page = lstpages[k]\n",
    "                    if link is None :\n",
    "\n",
    "                        liens = soup.find_all('a', href =True)\n",
    "                        nbliens=len(liens)\n",
    "                        compt=1\n",
    "                        while compt <= nbliens and link is None :\n",
    "                            tag=liens[nbliens-compt]\n",
    "                            if page in tag.descendants :\n",
    "                                link = tag\n",
    "\n",
    "                            compt += 1\n",
    "\n",
    "                    if link is None :\n",
    "\n",
    "                        liens = soup.find_all('a', href =True)\n",
    "                        nbliens=len(liens)\n",
    "                        compt=1\n",
    "                        while compt <= nbliens and link is None :\n",
    "                            tag=liens[nbliens-compt]\n",
    "                            for elem in tag.descendants :\n",
    "                                if elem.string is not None :\n",
    "                                    chaine = str(elem.string)\n",
    "                                    if chaine.find(page)!=-1 :\n",
    "                                        link = tag\n",
    "\n",
    "                            compt += 1\n",
    "\n",
    "                    if link is not None :\n",
    "\n",
    "                        newurl = link.get('href')\n",
    "\n",
    "                        if url[8:10]=='ww' :\n",
    "                            test=url[12:-4]\n",
    "\n",
    "                        else :\n",
    "                            test = url[8:-4]\n",
    "\n",
    "                        if newurl.find(test) == -1 and newurl[:2]!='ht' and newurl[:2]!='//': #cas où url relatif\n",
    "                            if url[-3:]=='/fr' and newurl[:3]=='/fr':\n",
    "                                newurl = url+newurl[3:]\n",
    "\n",
    "                            elif url[-4:]=='/fr/' and newurl[:3]=='/fr' :\n",
    "                                newurl = url + newurl[4:]\n",
    "\n",
    "                            else :\n",
    "                                newurl = url + newurl\n",
    "\n",
    "                        if newurl[:2]=='//':\n",
    "                            newurl='http:'+newurl\n",
    "\n",
    "                        newres = requests.get(newurl)\n",
    "                        contenu = str(newres.content)\n",
    "\n",
    "                        #siren = findSIREN(contenu)\n",
    "\n",
    "                        #sirentrouve = isSIREN(siren)\n",
    "\n",
    "                        #if not sirentrouve : #gérons les espaces insécables\n",
    "\n",
    "                        soup2=BeautifulSoup(newres.content, 'html.parser')\n",
    "                        chaine2=str(soup2)\n",
    "                        chaine3=chaine2.replace('\\xa0', ' ')\n",
    "\n",
    "                        siren = findSIREN(chaine3)\n",
    "                        sirentrouve=isSIREN(siren)\n",
    "\n",
    "                    k+=1\n",
    "            newfile.write(str.join(';', [lst[0],siren]))\n",
    "            newfile.write('\\n')\n",
    "\n",
    "    except :\n",
    "        newfile.write(str.join(';', [lst[0],'erreur de connection']))\n",
    "        newfile.write('\\n')\n",
    "    i+=1\n",
    "\n",
    "\n",
    "file.close()\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3) :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
